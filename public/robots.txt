# robots.txt - Controls search engine crawling
# This helps prevent certain types of automated scraping

User-agent: *
Allow: /

# Prevent crawling of admin or sensitive areas (if any)
Disallow: /api/
Disallow: /.well-known/

# Sitemap location (update with your actual domain before deploying)
# TODO: Update this to your actual domain (e.g., https://aquablast.com/sitemap.xml)
Sitemap: https://yourdomain.com/sitemap.xml

# Crawl-delay to prevent aggressive crawling (helps reduce load)
Crawl-delay: 10

# Block known bad bots (aggressive scrapers, spammers)
User-agent: AhrefsBot
Crawl-delay: 30

User-agent: SemrushBot
Crawl-delay: 30

User-agent: DotBot
Crawl-delay: 30

User-agent: MJ12bot
Crawl-delay: 30

# Block AI training bots if desired
User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /

User-agent: Claude-Web
Disallow: /
